\documentclass{anstrans}[12pt]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{FluDAG and Other Implementations of the DAGMC Toolkit}
\author{Paul P.H. Wilson, Andrew Davis, Julie Zachman, Kerry L. Dunn}

\institute{University of Wisconsin-Madison, 1500 Engineering Dr,
  Madison, WI 53706}

\email{wilsonp@engr.wisc.edu}

% Optional disclaimer: remove this command to hide

%%%% packages and definitions (optional)
\usepackage{graphicx} % allows inclusion of graphics
\usepackage{epsfig}
\usepackage{booktabs} % nice rules (thick lines) for tables
\usepackage{microtype} % improves typography for PDF

\def\C++{%
    C\kern-.1667em\raise.30ex\hbox{\small{+\kern-.1667em+}}%
\spacefactor1000 }

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

The Direct Accelerated Geometry Monte Carlo (DAGMC)\cite{dagmc} toolkit provides
an interface to the Mesh-Oriented datABase (MOAB)\cite{MOAB} specialized for the
geometric operations of Monte Carlo radiation transport directly on
CAD-based geometries.  By taking full control of the geometric
operations, including ray tracing, point inclusion, and surface
normal determination, DAGMC allows an identical geometry representation to be
used in a variety of Monte Carlo physics solvers.

This work describes the application of the DAGMC toolkit to
Fluka\cite{fluka}, a Monte Carlo physics solver from CERN with support
for a wide array of particles at a broad range of energies.  The Fluka
implementation of DAGMC relies on previous work to integrate
GEANT4\cite{GEANT4} geometry capability with the Fluka physics solver,
known as FluGG\cite{flugg}.

In addition, we provide updates on efforts to integrate DAGMC with
other physics solvers including MCNP5\cite{mcnp5},
Tripoli4\cite{tripoli}, GEANT4, and Shift\cite{shift}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The DAGMC Toolkit}

The details of DAGMC have been described in other work\cite{dagmc}, but are
summarized here for context.  A user generates a geometric description
of the problem using one of a variety of solid modeling software
packages and imports that geometry into Cubit\cite{cubit} for
pre-processing steps including assignment of materials and boundary
condition, and imprinting and merging.  This geometry file is used to
generate a facetted representation of the geometry that is stored in a
MOAB format, with precision adequate to guarantee reliable radiation
transport results\cite{snouffer_ans}.  When loaded into a physics solver
modified with DAGMC, the material and boundary condition information
is extracted and an oriented bounding box tree is generated to enable
accelerated geometry operations on this representation, typically with $O(10^6)$
facets.  DAGMC allows simulation of complex geometries without
time-consuming translation to native geometric representations of the
physics solvers, with modest performance impact.  It also allows the
simulation of high-order surfaces that may not be supported by the
native geometric representation.

The DAGMC toolkit is comprised primarily of a small set of geometric
operations typical of Monte Carlo radiation transport: ray-tracing,
point inclusion, and normal determination.  The DAGMC interface also
includes methods for handling metadata such as material assignment
and boundary conditions.  Integration with a physics solver typically
requires identifying all the geometry queries made by the physics
solver and replacing them with queries to the DAGMC interface.  In
addition, various approaches are necessary to manage the
initialization phase of the simulation, depending on the physics
solver's initialization methods.  The primary physics solver used with
DAGMC has been MCNP5, providing unique computing capability for
analysis of shielding in fusion systems\cite{fusion3d} and criticality
of deformed space nuclear energy systems\cite{nets}.

Integration of DAGMC with a particular physics solver is generally
easiest when that solver has already abstracted its geometry
interface.  There are different levels of sophistication for such
abstractions, depending on previous efforts to allow alternative
geometry representations.  In its simplest form, this abstraction is a
set of function names reserved for use by external geometry
representations.  In its most advanced form, this abstraction uses
language paradigms to define formal interface that can be implemented
by any geometry representation.  Handling geometry and metadata
initialization is often the most challenging aspect of integrating
DAGMC with a physics solver.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Fluka, FluGG and FluDAG}

FLUKA\cite{fluka} is a Monte Carlo code provided by the Italian Institute for Nuclear
Physics (INFN) and the Center for European Nuclear Research (CERN). The
code is typically used for simulations in support of the facilities at   
CERN, ATLAS, LHC, etc, however is widely used for detector simulations and 
solar particle analysis. 

FLUGG\cite{flugg} is the FLUKA interface for Geant4 Geometry. It is a suite
of \C++ wrapper routines that acts as an interface between the exposed
parts of FLUKA subroutines and Geant4 libraries. The FLUKA team opened
a number of interfaces to the FLUKA code around specific routines to
allow FLUKA's physics routines to be used with Geant4
geometry and Geant4 geometric queries, such as the distance
to next surface and the volume of the other side of the next
surface. The user simply takes the \C++ routines that represent the
geometry, compile these to object files, link against a limited subset
of the Geant4 libraries and then link against the FLUKA libraries,
thus providing a custom executable containing the users geometry.

The FLUGG compilation has a main() routine that initializes the Geant4
geometry and calls the main FLUKA routine with a switch that signifies
that specific geometry calls should execute from the user-provided
wrapper functions. FLUGG input files are similar to FLUKA input, with a 
two-step workflow needed to capture material assignments.

\subsection{FluDAG Implementation \& Development}
The FluDAG development schema followed a similar implementation as
FLUGG using the exposed parts of the FLUKA interface. The wrapper
routines were implemented with the aid of DAGMC geometry calls.

\newpage
\C++ wrapper routines were written for the fundamental queries that 
must be implemented:

\begin{description}
	\item[g1wr]{Given the current ray volume ($V_i$), position
  ($x_i$,$y_i$,$z_i$), and direction
  ($\Omega_x$,$\Omega_y$,$\Omega_z$),return the distance to the next
  surface intersection.}
	\item[lkwr]{Given the ray position ($x_i$,$y_i$,$z_i$), return the
  volume that position belong to.}
	\item[nrmlwr]{When crossing from volume ($V_i$) to volume ($V_j$),
  return the vector between the particle direction and the surface
  normal.}
\end{description}

As with FLUGG, preparation of the input files is a two step process
that involves parsing the geometry input file to create material
assignments that are then pasted into the Fluka input deck for the main
program run.  There are some differences between FLUGG and FluDAG. 
The compiled executable is designed to take arbitrary input geometry meshes 
and thus there is no additional need for recompilation of the executable.
FluDAG has no knowledge of material compositions and thus cannot produce
detail material description, but does handle the material assignments.
%make use of material information and contains no
%methods that could be modified to produce the material assignments
%required by FLUKA input.  A separate preprocessing program has been
%modified to perform this task.

\section{FluDAG Testing}
The testing of the code centered around the comparison between
simulations performed using native Fluka geometries and the FluDAG
equivalent. The models were built using native MCNP geometries;
converted to CAD using mcnp2cad for FluDAG, and converted to Fluka by
importing the MCNP geometry using the Flair\cite{flair} interface. Each test
is validating a specific aspect of FluDAG to ensure correctness. We
can broadly classify the testing into analytic tests, where we can
derive the expected result from first principles and then code tests.

\subsection{Analytic Tests}
Analytic testing of Monte Carlo codes is preferred over code testing since the
answer to be compared against is exact and thus any discrepancy is due
to the implementation as opposed to the statistics or the nuclear data
involed in the calculation. The first set of tests are to show that in
vacuum calculations of well defined geometries the FluDAG codes gives
the same result as expected by theory and by Fluka.

\subsubsection*{Track Length based tests}
There were several analytic vacuum test problems to prove that results
between Fluka and FluDAG are identical. The first test problems
demonstrate that Fluka's track length based esimator, USRTRACK, 
scores are producing identical scores, the test model composed of 
10 cubes of side 10.0 cm joined in a line by opposing faces. A plane 
beam of neutrons start particles at an x coordinate of 0.0 cm. Setting 
the volume of each scorer to be 1.0 cm$^3$, should result in a score of 
10.0 for each tally. Both Fluka and FluDAG report a score of 9.999996 for every volume,
showing that our implementation is correct and for this cubic model
shows that results are identical.
%% \begin{table}%[h!]
%% 	\begin{center}
%% 		\begin{tabular}{|l|c|c|c|}
%% 			\hline
%% 			Cell \# & Expected Score & Fluka  & FluDAG \\
%% 			\hline
%% 			1 & 10.0 & 9.999996 & 9.999996 \\
%% 			... & ... & ... & ... \\
%% 			10 & 10.0 & 9.999996 & 9.999996 \\
%% 			\hline
%% 		\end{tabular}
%% 	\caption{List of volumewise track length fluxes that are 
%% 				expected by theory, Fluka and FluDAG. Note, the ``...''
%% 				is intended to mean the same as above.}\label{table:usrtrack_comp}

%% 	\end{center}
%% \end{table}

In order to show the expected level of deviation when we consider
curved surfaces, a further model was created composed of 3 nested
spheres, such that the incremental radius of each sphere is 10.0\ cm,
hence the first volume is a sphere and subsequent volumes are
spherical shells. The particle source was defined to be an
isotropically emitting point source at the centre of the
model. Normalising each tally such that the volume is set to 1.0 cm$^3$, 
we expect that the score for each tally should be the same as the
incremental radius of the volume, i.e. 10.0 cm. Running the calculation
in both Fluka and FluDAG we get the results shown in Table \ref{usrtrack_comp_sphere}.

\begin{table}%[h!]
	\begin{center}
		\begin{tabular}{|l|c|c|c|}
			\hline
			Cell \# & Expected Score & Fluka  & FluDAG \\
			\hline
			1 & 10.0 & 9.999996 & 9.999209 \\
			2 & 10.0 & 9.999996 & 10.00000 \\
			3 & 10.0 & 9.999996 & 9.999995 \\
			\hline
		\end{tabular}
		\caption{List of volumewise track length fluxes that are 
			expected by theory, Fluka and FluDAG}\label{usrtrack_comp_sphere}

	\end{center}
\end{table}

The variation between Fluka and FluDAG results are no larger than the
$4^{th}$ significant figure, this is commensurate with previous
studies \cite{snouffer_ans} showing that faceting to a accuracy of $10^{-n}$ can
effect the $n^{th}$ significant figure. 

\subsubsection*{Surface Flux and current tests}
The testing of the Boundary Crossing scoring began with using a model
comprising of a 5 cm diameter cylinder, subdivided into 9 regions by
placing a plane in 8 separate locations along the length of the
cylinder. The model is taken to be composed of vacuum, and a disk
source of neutrons producing a parallel beam of particles is placed at
0.0 cm. Each surface that is used to define the boundary between
volumes is defined to be inclined at an angle of 10$^{\circ}$ with respect to the
last, the first surface is inclined at 10$^{\circ}$, the second
surface at 20$^{\circ}$ and so on upto 80$^{\circ}$.

%\begin{figure}[h!]
%	\begin{center}
%		\epsfig{file=figs/usrbdx_geom.ps,width=.6\columnwidth}
%		\caption{The model used showing the 8 separating surfaces each
%		incremented at an angle 10 $^{\circ}$ to the last}
%	\end{center}
%\end{figure}

\begin{table}%[h!]
        \begin{center}
                \begin{tabular}{|l|c|c|c|}
                  \hline
                        Angle & Expected Score & Fluka  & FluDAG \\
                        \hline
                        10$^{\circ}$ & 1.015426 & 1.015426 & 1.015407\\
                        20$^{\circ}$ & 1.064178 & 1.064177 & 1.064157\\
                        30$^{\circ}$ & 1.154700 & 1.154700 & 1.154677\\
                        40$^{\circ}$ & 1.305407 & 1.305407 & 1.305381\\
                        50$^{\circ}$ & 1.555724 & 1.555723 & 1.555693\\
                        60$^{\circ}$ & 2.000000 & 1.999999 & 1.999960\\
                        70$^{\circ}$ & 2.923804 & 2.923803 & 2.923746\\
                        80$^{\circ}$ & 5.758770 & 5.758769 & 5.758655\\
                        \hline
                \end{tabular}
                \caption{List of surface fluxes as a function of surface
                         angle as expected by theory and as calculated by 
                         Fluka and FluDAG}
                \label{usrbdx_crossing}
                \end{center}
\end{table}

As shown in Table~\ref{table:usrbdx_crossing} the results are as defined as expected, 
there are some numerical differences beyond the $4^{th}$ significant figure but given that the model
was faceted to a tolerance of $10^{-4}$ we can expect no better accuracy 
than 4 significant figure.

\subsection{Code Comparisons}
\subsubsection*{Surface flux with particle scattering}
Having performed analytic comparisons there is higher confidence in
the implementation of FluDAG and thus code to code comparisons can be
performed. In order to prove the USRBDX score between two volumes, a
model was created with a sphere of radius 50.0 cm composed of heavy
water, with particles being killed once they leave the sphere. An
isotropic point source was placed at the centre of the sphere. A
USRBDX score was created in both Fluka and FluDAG and scored the
neutron flux crossing the sphere in both angle and energy, there were
60 angular and 260 energy bins in the score.

\begin{figure}%[h!]
	\begin{center}
		\epsfig{file=figs/bin_ratio.ps,width=.6\columnwidth,angle=-90}
		\epsfig{file=figs/bin_ratio_2.ps,width=.6\columnwidth,angle=-90}
		\caption{The neutron current crossing the surface between volumes, in the 
		forward direction (upper) and at 80 degrees to the surface (lower)}\label{fig:mat_usrbdx}
	\end{center}
\end{figure}

The results shown in Figure \ref{fig:mat_usrbdx} show the ratio of the
flux of neutrons crossing the surface between the D$_2$O and vacuum
for Fluka and FluDAG. The upper part of Figure \ref{fig:mat_usrbdx} shows
the ratio of results for the first angular bin corresponding to $0
\to 0.209$ Steradians, and the lower part of Figure \ref{fig:mat_usrbdx}
shows the ratio of results for the $20^{th}$ angular corresponding to $3.979
\to 4.189$ steradians. We see that for both figures, the ratio of the
results as a function of energy agree to within $\pm 3\sigma$. That is, we
expect 99.7\% of results to lie within the area defined by the two
trend lines, when the estimate of the standard deviation is appropriate.

\subsubsection*{USRBIN mesh scoring including materials}
Having determined the accuracy and functionality of the most important 
volume based scoring methods, the remaining geometry independent methods
should also be tested.
Fluka has the USRBIN score, which allows arbitrary scoring
of data on a mesh superimposed on top of the geometry. In order to test this, a geometry
was constructed in which the axis of a cylinder of radius 10.0 cm points along the
+x axis, with planes at 0.0 and 0.1 cm define a vacuum region and 0.1 to 0.3 
defines a water region. A disk source located at x=0.001 of radius provides a 
parallel beam of 1 GeV photons.

\begin{figure}%[h!]
	\begin{center}
		\epsfig{file=figs/mesh_phot.ps,width=.6\columnwidth,angle=-90}
		\epsfig{file=figs/mesh_phot_zoom.ps,width=.6\columnwidth,angle=-90}
		\caption{USRBIN mesh scoring photon flux in increments
                  of $3.0 \times 10^{-5}$ cm, upper figure shows the ratio between
                  Fluka and FluDAG between 0.7 and 1.3, the lower
                  figure magnifies the range near 1.0. }\label{fig:mesh_phot}
	\end{center}
\end{figure}

As can be seen in the upper part of Figure \ref{fig:mesh_phot} the area behind the source 
(x < 0.01 cm) shows the poorest agreement, however the variation is well within the 
bounds defined by $\pm 3\sigma$. The large deviation away from the average is due to 
the flux behind the source being dominated by scattered photons, and thus are low in 
number as compared to the number of source particles.  The lower part of Figure 
\ref{fig:mesh_phot} shows the ratio between Fluka and FluDAG result, the variation seen is 
due to rounding in the last significant figure of the output mesh since the Fluka mesh format is written 
 in the format E11.4. 

\subsubsection{Examples of complex FLuDAG models}
Due to the early stage of development few large FluDAG calculations have been performed, however
one of the NASA-provided test geometries is shown below in Figure \ref{nasa_mesh}.  This calculation demonstrated 
that in a model composed entirely of vacuum no particles were lost and no oscillations were
introduced into the particle flux.

\begin{figure}%[h!]
        \begin{center}
                \epsfig{file=figs/nasa_mesh.ps,width=.6\columnwidth,angle=90}
                \caption{Neutron flux in a void run of the NASA test problem, 
                         faceted to $10\times10^{-4}$, point source at centre of model}\label{nasa_mesh}
        \end{center}
\end{figure}

\section{Other DAGMC Implementations}

The DAGMC toolkit has already been integrated with a variety of
physics solvers.  The earliest development of DAGMC relied on source
code for MCNPX v2.x as testbed for capability development.  The focus
was shifted to MCNP5 and this has been the primary development
platform and the product, DAG-MCNP5, is used routinely for production
calculations in fusion neutronics applications.  The geometry
interface in MCNP5, written in FORTRAN90, is the least abstracted of
all the current implementations, requiring relatively intrusive source
code patches.  DAGMC will be integrated with MCNP6 when its final source
code becomes available.

DAGMC has also been integrated with Tripoli4\cite{tripoli} from CEA
Saclay.  Due to prior abstraction of the geometry interface to support
the ROOT geometry representation, in addition to the native Tripoli
representation, this integration was relatively straightforward.
Tripoli exposes a well defined \C++ interface for determining the
distance to the next surface, whether or not a point is inside a
volume, what volume is entered upon crossing a surface, and what the
normal is on a surface.

A recent effort to integrate DAGMC with GEANT4 has also proven
successful \cite{dag_geant4}.  GEANT4 has a highly abstracted geometry
interface, with a different implementations included with the base
GEANT4 package.  This implementation extends the standard G4VSolid
\C++ base class to create a DagSolid, with performance that compares
favorably to the G4TesselatedSolid class that provides a similar
capability.

The DAGMC toolkit has most recently been integrated with the Shift
physics solver from ORNL.  Similar to other modern \C++ solvers, Shift
presents a well-defined geometry abstraction that facilitates
integration, including metadata handling.  Different geometry
packages simply need to implement a common geometry interface, which
is templated on a geometric state that is unique to each package.
While unit testing has demonstrated a successful implementation,
final integration testing is still underway.

\section{Conclusions and Future Work}

The implementation of FluDAG, the combination of the DAGMC toolkit
with the FLUKA physics solver has been demonstrated successfully.  CAD
geometries, populated with material assignment metadata, can be
processed by FluDAG to generate identical tally results to those
generated with the native Fluka geometry.  Future efforts will improve
the usability of the FluDAG tool, verify additional features such as
magnetic fields, and incorporate the ability to tally on unstructure
tetrahedral mesh.

Having an identical geometry representation available across a broad
set of physics solvers will enable a more robust comparison of the
physics methods and data libraries of each tool.  A future effort will
devise a suite of test problems that will be used with each of the
physics solvers and their results compared.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Acknowledgments}
This work is supported in part by NASA under Wyle contract T72277.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{ans}
\bibliography{bibliography}
\end{document}

